# WD14 Batch Processing Implementation

**Date:** 2026-01-04  
**Status:** Functional âœ…  
**Last Updated By:** Claude

## Overview
The WD14 Tagger backend supports efficient batch processing of images, allowing multiple images to be tagged in a single inference call. This significantly improves throughput when processing queues of images.

**Fast TAG Mode** automatically uses the WD14 model for optimal performance, regardless of the user's configured tagging model. This ensures consistency and maximum speed during batch operations.

## Architecture

### 1. Backend Implementation (`wd14_backend/backend.py`)

The WD14 backend's `caption()` method automatically detects and handles batch requests:

```python
def caption(self, image, length="normal", stream=False, settings=None):
    # Check if input is list (Batch Mode)
    is_batch = isinstance(image, list)
    images = image if is_batch else [image]
    
    # ViTImageProcessor handles lists automatically
    inputs = self.processor(images=images, return_tensors="pt").to(self.device)
    
    with torch.no_grad():
        outputs = self.model(**inputs)
    
    # Process results for each image
    batch_results = []
    for i in range(len(images)):
        # ... process tags for each image ...
        batch_results.append({"text": text})
    
    # Return single dict if single input (backward compatibility)
    if not is_batch:
        return batch_results[0]
        
    # Return list if batch input
    return batch_results
```

**Key Features:**
- Automatic batch detection via `isinstance(image, list)`
- Backward compatible - returns single dict for single images
- Returns list of dicts for batch processing
- Uses PyTorch's native batch processing capabilities

### 2. API Endpoint (`moondream_station/core/routers/vision.py`)

A dedicated `/v1/vision/batch-caption` endpoint handles batch requests:

```python
@router.post("/batch-caption")
async def batch_caption(request: Request):
    \"\"\"
    Batch caption images using WD14 Tagger
    Accepts: { "images": ["b64...", ...], "model": "..." }
    \"\"\"
    # Get base64 images from request
    images_b64 = data.get("images", [])
    model_id = data.get("model", config.get("current_model"))
    
    # Decode all images to PIL
    pil_images = []
    for b64_str in images_b64:
        # Handle data URI or raw base64
        if b64_str.startswith("data:image"):
            _, encoded = b64_str.split(",", 1)
        else:
            encoded = b64_str
        
        raw_bytes = base64.b64decode(encoded)
        img = Image.open(io.BytesIO(raw_bytes)).convert("RGB")
        pil_images.append(img)
    
    # Execute batch inference
    captions = await inference_service.execute_function("caption", image=pil_images)
    
    return {
        "captions": captions,  # List of {"text": "tags, ..."} dicts
        "count": len(captions),
        "duration": round(duration, 3)
    }
```

### 3. Frontend Integration (`services/providers/moondream/MoondreamLocalProvider.ts`)

The frontend provider sends batch requests:

```typescript
async batchTagImages(images: ImageInfo[], settings: AdminSettings): Promise<{ tags: string[], imageId: string }[]> {
    const apiUrl = `${baseUrl}/v1/vision/batch-caption`;
    
    const body = {
        model: modelOverride || "wd-vit-tagger-v3",
        images: images.map(img => img.dataUrl.split(',')[1])  // Strip data URI header
    };
    
    const result = await callMoondreamApi(apiUrl, "", body, false, 120, vramMode);
    const json = JSON.parse(result.text);
    
    return json.captions.map((c: any, idx: number) => ({
        imageId: images[idx].id,
        tags: c.text.split(',').map((t: string) => t.trim())
    }));
}
```

### 4. Queue Integration (`hooks/queue/useQueueProcessor.ts`)

The queue processor automatically batches consecutive analysis tasks:

```typescript
// BATCH MODE LOGIC
if (isBatchMode && task.taskType === 'analysis') {
    // Try to fill the batch
    for (let i = 0; i < BATCH_SIZE - 1; i++) {
        if (queueRef.current.length > 0 && queueRef.current[0].taskType === 'analysis') {
            const next = queueRef.current.shift();
            if (next) currentBatch.push(next);
        }
    }
}

// Execute batch if multiple items
if (currentBatch.length > 1 && task.taskType === 'analysis') {
    await executeBatchAnalysis(currentBatch);
} else {
    // Single execution fallback
    await executeAnalysis(task);
}
```

## Performance Characteristics

### Optimal Batch Size
- **Recommended:** 4 images per batch
- **Maximum:** 8 images (hardware dependent)
- Determined by `useVRAMManagement.ts` based on available VRAM

### Memory Usage
- **VRAM:** ~400MB base model + ~50MB per image in batch
- **System RAM:** ~2GB base + ~100MB per high-res image
- Images are decoded to PIL format in system RAM before GPU transfer

### Speed Improvements
- **Single Processing:** ~500ms per image
- **Batch (4 images):** ~800ms total (~200ms per image)
- **Throughput Increase:** ~2.5x faster

## Known Issues & Limitations

### 1. System RAM Pressure
**Symptom:** Process killed with "Killed" message  
**Cause:** Excessive model discovery scans + batch processing overwhelming RAM  
**Workarounds:**
- Reduce batch size to 2-3 images
- Limit concurrent requests
- Cache model discovery results

### 2. Model Discovery Spam
**Symptom:** Repeated "DEBUG: Discovering models" in logs  
**Cause:** `/v1/models` endpoint being polled too frequently  
**Impact:** High CPU usage, RAM consumption from filesystem scans  
**Fix:** Implement caching in `routers/models.py`

### 3. WD14 Model Version Mismatch
**Note:** Backend currently hardcodes `wd-v1-4-vit-tagger-v2`  
**Frontend requests:** `wd-vit-tagger-v3`  
**Impact:** Works fine, just uses v2 regardless of request  
**TODO:** Add `init_backend()` method to accept model_id parameter

### 4. Caption vs Tagging Distinction
**Behavior:** Batch mode only performs tagging, skips detailed captioning  
**Reason:** WD14 is a tagging model, not a captioning model  
**Documented:** User-facing tooltip explains this limitation

## Troubleshooting

### Batch Processing Fails with 503
1. Check server logs for "Killed" message (OOM)
2. Verify VRAM usage is low (< 50%)
3. Check system RAM usage with `free -h`
4. Reduce batch size in frontend
5. Stop other heavy processes

### No Speed Improvement
1. Verify "Batch Tag" button is ON in Queue Monitor
2. Check that queue has multiple consecutive analysis tasks
3. Monitor logs for "Batch=X" to confirm batching is active
4. Ensure WD14 model is loaded (not Moondream)

### Images Not Being Batched
1. Queue must have multiple items of `taskType: 'analysis'`
2. Items must be consecutive (not interleaved with generation tasks)
3. Batch mode must be enabled

## Future Improvements

1. **Cache Model Discovery** - Cache `/v1/models` response for 60 seconds
2. **Dynamic Model Loading** - Support switching between WD14 v2/v3
3. **Memory Profiling** - Add RAM usage tracking alongside VRAM
4. **Adaptive Batch Sizing** - Automatically reduce batch size on OOM
5. **Image Preprocessing** - Resize large images before batching

## Testing

### Manual Test
```bash
# Start backend
cd ~/.moondream-station/moondream-station
python3 start_server.py

# Test batch endpoint
curl -X POST http://localhost:2020/v1/vision/batch-caption \
  -H "Content-Type: application/json" \
  -d '{
    "model": "wd14-vit-v2",
    "images": ["BASE64_1", "BASE64_2", "BASE64_3"]
  }'
```

### Expected Response
```json
{
  "captions": [
    {"text": "1girl, solo, long hair, blue eyes"},
    {"text": "landscape, outdoors, sky, mountain"},
    {"text": "cat, animal, cute, whiskers"}
  ],
  "count": 3,
  "duration": 0.753
}
```

## References
- WD14 Model: https://huggingface.co/SmilingWolf/wd-v1-4-vit-tagger-v2
- Transformers Batch Processing: https://huggingface.co/docs/transformers/main_classes/processors
- Queue System Documentation: `docs/QUEUE_REFACTOR_COMPLETE.md`
